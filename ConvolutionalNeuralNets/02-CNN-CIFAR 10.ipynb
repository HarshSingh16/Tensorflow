{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-Project-Exercise\n",
    "We'll be using the CIFAR-10 dataset, which is very famous dataset for image recognition! \n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "### Follow the Instructions in Bold, if you get stuck somewhere, view the solutions video! Most of the challenge with this project is actually dealing with the data and its dimensions, not from setting up the CNN itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Get the Data\n",
    "\n",
    "** *Note: If you have trouble with this just watch the solutions video. This doesn't really have anything to do with the exercise, its more about setting up your data. Please make sure to watch the solutions video before posting any QA questions.* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Download the data for CIFAR from here: https://www.cs.toronto.edu/~kriz/cifar.html **\n",
    "\n",
    "**Specifically the CIFAR-10 python version link: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz **\n",
    "\n",
    "** Remember the directory you save the file in! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put file path as a string here\n",
    "CIFAR_DIR = 'cifar-10-batches-py/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive contains the files data_batch_1, data_batch_2, ..., data_batch_5, as well as test_batch. Each of these files is a Python \"pickled\" object produced with cPickle. \n",
    "\n",
    "** Load the Data. Use the Code Below to load the data: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        cifar_dict = pickle.load(fo, encoding='bytes')\n",
    "    return cifar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,direc in zip(all_data,dirs):\n",
    "    all_data[i] = unpickle(CIFAR_DIR+direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_meta = all_data[0]\n",
    "data_batch1 = all_data[1]\n",
    "data_batch2 = all_data[2]\n",
    "data_batch3 = all_data[3]\n",
    "data_batch4 = all_data[4]\n",
    "data_batch5 = all_data[5]\n",
    "test_batch = all_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'num_cases_per_batch': 10000,\n",
       " b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Why the 'b's in front of the string? **\n",
    "Bytes literals are always prefixed with 'b' or 'B'; they produce an instance of the bytes type instead of the str type. They may only contain ASCII characters; bytes with a numeric value of 128 or greater must be expressed with escapes.\n",
    "\n",
    "https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "* data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "* labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
    "\n",
    "* label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a single image using matplotlib.\n",
    "\n",
    "** Grab a single image from data_batch1 and display it with plt.imshow(). You'll need to reshape and transpose the numpy array inside the X = data_batch[b'data'] dictionary entry.**\n",
    "\n",
    "** It should end up looking like this: **\n",
    "\n",
    "    # Array of all images reshaped and formatted for viewing\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2688b3cdd68>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put the code here that transforms the X array!\n",
    "X=data_batch1[b'data'].reshape(10000,3,32,32).transpose(0,2,3,1)\n",
    "plt.imshow(X[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Dealing With Data.\n",
    "\n",
    "** Use the provided code below to help with dealing with grabbing the next batch once you've gotten ready to create the Graph Session. Can you break down how it works? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vec, vals=10):\n",
    "    '''\n",
    "    For use to one-hot encode the 10- possible labels\n",
    "    '''\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        \n",
    "        self.all_train_batches = [data_batch1,data_batch2,data_batch3,data_batch4,data_batch5]\n",
    "        self.test_batch = [test_batch]\n",
    "        \n",
    "        self.training_images = None\n",
    "        self.training_labels = None\n",
    "        \n",
    "        self.test_images = None\n",
    "        self.test_labels = None\n",
    "    \n",
    "    def set_up_images(self):\n",
    "        \n",
    "        print(\"Setting Up Training Images and Labels\")\n",
    "        \n",
    "        self.training_images = np.vstack([d[b\"data\"] for d in self.all_train_batches])\n",
    "        train_len = len(self.training_images)\n",
    "        \n",
    "        self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        self.training_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.all_train_batches]), 10)\n",
    "        \n",
    "        print(\"Setting Up Test Images and Labels\")\n",
    "        \n",
    "        self.test_images = np.vstack([d[b\"data\"] for d in self.test_batch])\n",
    "        test_len = len(self.test_images)\n",
    "        \n",
    "        self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        self.test_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.test_batch]), 10)\n",
    "\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)\n",
    "        y = self.training_labels[self.i:self.i+batch_size]\n",
    "        self.i = (self.i + batch_size) % len(self.training_images)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How to use the above code: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Training Images and Labels\n",
      "Setting Up Test Images and Labels\n"
     ]
    }
   ],
   "source": [
    "# Before Your tf.Session run these two lines\n",
    "ch = CifarHelper()\n",
    "ch.set_up_images()\n",
    "\n",
    "# During your session to grab the next batch use this line\n",
    "# (Just like we did for mnist.train.next_batch)\n",
    "# batch = ch.next_batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "** Import tensorflow **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create 2 placeholders, x and y_true. Their shapes should be: **\n",
    "\n",
    "* x shape = [None,32,32,3]\n",
    "* y_true shape = [None,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,32,32,3])\n",
    "y_true=tf.placeholder(tf.float32,shape=[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create one more placeholder called hold_prob. No need for shape here. This placeholder will just hold a single probability for the dropout. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "** Grab the helper functions from MNIST with CNN (or recreate them here yourself for a hard challenge!). You'll need: **\n",
    "\n",
    "* init_weights\n",
    "* init_bias\n",
    "* conv2d\n",
    "* max_pool_2by2\n",
    "* convolutional_layer\n",
    "* normal_full_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Weight Function\n",
    "def init_weights(shape):\n",
    "    init_random_dist=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Bias Function\n",
    "def init_bias(shape):\n",
    "    init_bias_rand=tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init_bias_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the conv2d\n",
    "def conv2d(x,w):\n",
    "    return tf.nn.conv2d(x,w,strides=[1,1,1,1],padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating max_pool_2by2\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x,\n",
    "                         ksize=[1,2,2,1],\n",
    "                         strides=[1,2,2,1],\n",
    "                         padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Convolutional Layer\n",
    "def convolutional_layer(input_x,shape):\n",
    "    w=init_weights(shape)\n",
    "    b=init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x,w)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Normal Layer\n",
    "def normal_layer(input_layer,size):\n",
    "    input_size=int(input_layer.get_shape()[1])\n",
    "    w=init_weights([input_size,size])\n",
    "    b=init_bias([size])\n",
    "    return tf.matmul(input_layer,w)+b    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Layers\n",
    "\n",
    "** Create a convolutional layer and a pooling layer as we did for MNIST. **\n",
    "** Its up to you what the 2d size of the convolution should be, but the last two digits need to be 3 and 32 because of the 3 color channels and 32 pixels. So for example you could use:**\n",
    "\n",
    "        convo_1 = convolutional_layer(x,shape=[4,4,3,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_1 = convolutional_layer(x,shape=[4,4,3,32])\n",
    "convo_1_pooling=max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create the next convolutional and pooling layers.  The last two dimensions of the convo_2 layer should be 32,64 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_2 = convolutional_layer(convo_1_pooling,shape=[4,4,32,64])\n",
    "convo_2_pooling=max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now create a flattened layer by reshaping the pooling layer into [-1,8 \\* 8 \\* 64] or [-1,4096] **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*8*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convo flatting\n",
    "convo_flat=tf.reshape(convo_2_pooling,[-1,8*8*64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a new full layer using the normal_full_layer function and passing in your flattend convolutional 2 layer with size=1024. (You could also choose to reduce this to something like 512)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a normal full layer\n",
    "full_layer_one=tf.nn.relu(normal_layer(convo_flat,1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer=tf.nn.dropout(full_layer_one,hold_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finally set the output to y_pred by passing in the dropout layer into the normal_full_layer function. The size should be 10 because of the 10 possible labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=normal_layer(dropout_layer,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "** Create a cross_entropy loss function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "** Create the optimizer using an Adam Optimizer. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train=optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a variable to intialize all the global tf variables. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Session\n",
    "\n",
    "** Perform the training and test print outs in a Tf session and run your model! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step 0\n",
      "Accuracy is:\n",
      "0.1389\n",
      "\n",
      "\n",
      "Currently on step 100\n",
      "Accuracy is:\n",
      "0.413\n",
      "\n",
      "\n",
      "Currently on step 200\n",
      "Accuracy is:\n",
      "0.4753\n",
      "\n",
      "\n",
      "Currently on step 300\n",
      "Accuracy is:\n",
      "0.4976\n",
      "\n",
      "\n",
      "Currently on step 400\n",
      "Accuracy is:\n",
      "0.5269\n",
      "\n",
      "\n",
      "Currently on step 500\n",
      "Accuracy is:\n",
      "0.5452\n",
      "\n",
      "\n",
      "Currently on step 600\n",
      "Accuracy is:\n",
      "0.5641\n",
      "\n",
      "\n",
      "Currently on step 700\n",
      "Accuracy is:\n",
      "0.5848\n",
      "\n",
      "\n",
      "Currently on step 800\n",
      "Accuracy is:\n",
      "0.5954\n",
      "\n",
      "\n",
      "Currently on step 900\n",
      "Accuracy is:\n",
      "0.5988\n",
      "\n",
      "\n",
      "Currently on step 1000\n",
      "Accuracy is:\n",
      "0.6184\n",
      "\n",
      "\n",
      "Currently on step 1100\n",
      "Accuracy is:\n",
      "0.6152\n",
      "\n",
      "\n",
      "Currently on step 1200\n",
      "Accuracy is:\n",
      "0.6285\n",
      "\n",
      "\n",
      "Currently on step 1300\n",
      "Accuracy is:\n",
      "0.6144\n",
      "\n",
      "\n",
      "Currently on step 1400\n",
      "Accuracy is:\n",
      "0.6373\n",
      "\n",
      "\n",
      "Currently on step 1500\n",
      "Accuracy is:\n",
      "0.6315\n",
      "\n",
      "\n",
      "Currently on step 1600\n",
      "Accuracy is:\n",
      "0.6392\n",
      "\n",
      "\n",
      "Currently on step 1700\n",
      "Accuracy is:\n",
      "0.6512\n",
      "\n",
      "\n",
      "Currently on step 1800\n",
      "Accuracy is:\n",
      "0.6554\n",
      "\n",
      "\n",
      "Currently on step 1900\n",
      "Accuracy is:\n",
      "0.663\n",
      "\n",
      "\n",
      "Currently on step 2000\n",
      "Accuracy is:\n",
      "0.658\n",
      "\n",
      "\n",
      "Currently on step 2100\n",
      "Accuracy is:\n",
      "0.6619\n",
      "\n",
      "\n",
      "Currently on step 2200\n",
      "Accuracy is:\n",
      "0.6669\n",
      "\n",
      "\n",
      "Currently on step 2300\n",
      "Accuracy is:\n",
      "0.6698\n",
      "\n",
      "\n",
      "Currently on step 2400\n",
      "Accuracy is:\n",
      "0.6712\n",
      "\n",
      "\n",
      "Currently on step 2500\n",
      "Accuracy is:\n",
      "0.6515\n",
      "\n",
      "\n",
      "Currently on step 2600\n",
      "Accuracy is:\n",
      "0.6751\n",
      "\n",
      "\n",
      "Currently on step 2700\n",
      "Accuracy is:\n",
      "0.6856\n",
      "\n",
      "\n",
      "Currently on step 2800\n",
      "Accuracy is:\n",
      "0.6753\n",
      "\n",
      "\n",
      "Currently on step 2900\n",
      "Accuracy is:\n",
      "0.6781\n",
      "\n",
      "\n",
      "Currently on step 3000\n",
      "Accuracy is:\n",
      "0.6856\n",
      "\n",
      "\n",
      "Currently on step 3100\n",
      "Accuracy is:\n",
      "0.6696\n",
      "\n",
      "\n",
      "Currently on step 3200\n",
      "Accuracy is:\n",
      "0.69\n",
      "\n",
      "\n",
      "Currently on step 3300\n",
      "Accuracy is:\n",
      "0.6723\n",
      "\n",
      "\n",
      "Currently on step 3400\n",
      "Accuracy is:\n",
      "0.6849\n",
      "\n",
      "\n",
      "Currently on step 3500\n",
      "Accuracy is:\n",
      "0.6803\n",
      "\n",
      "\n",
      "Currently on step 3600\n",
      "Accuracy is:\n",
      "0.6754\n",
      "\n",
      "\n",
      "Currently on step 3700\n",
      "Accuracy is:\n",
      "0.6862\n",
      "\n",
      "\n",
      "Currently on step 3800\n",
      "Accuracy is:\n",
      "0.6738\n",
      "\n",
      "\n",
      "Currently on step 3900\n",
      "Accuracy is:\n",
      "0.6931\n",
      "\n",
      "\n",
      "Currently on step 4000\n",
      "Accuracy is:\n",
      "0.6838\n",
      "\n",
      "\n",
      "Currently on step 4100\n",
      "Accuracy is:\n",
      "0.6706\n",
      "\n",
      "\n",
      "Currently on step 4200\n",
      "Accuracy is:\n",
      "0.676\n",
      "\n",
      "\n",
      "Currently on step 4300\n",
      "Accuracy is:\n",
      "0.6742\n",
      "\n",
      "\n",
      "Currently on step 4400\n",
      "Accuracy is:\n",
      "0.6987\n",
      "\n",
      "\n",
      "Currently on step 4500\n",
      "Accuracy is:\n",
      "0.6891\n",
      "\n",
      "\n",
      "Currently on step 4600\n",
      "Accuracy is:\n",
      "0.6779\n",
      "\n",
      "\n",
      "Currently on step 4700\n",
      "Accuracy is:\n",
      "0.6817\n",
      "\n",
      "\n",
      "Currently on step 4800\n",
      "Accuracy is:\n",
      "0.686\n",
      "\n",
      "\n",
      "Currently on step 4900\n",
      "Accuracy is:\n",
      "0.6827\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5000):\n",
    "        batch = ch.next_batch(100)\n",
    "        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "            print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
